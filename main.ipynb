{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c105ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# img = Image.open(r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\3.2-DataIngestion\\page3_img1.png\").convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb561df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name facebook/dino-vits16. Creating a new one with mean pooling.\n",
      "d:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\gen\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kumaw\\.cache\\huggingface\\hub\\models--facebook--dino-vits16. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "<class 'transformers.models.vit.configuration_vit.ViTConfig'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m             names.append(file)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m images, names\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/dino-vits16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimage_to_vector\u001b[39m(image):\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.encode(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\gen\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:339\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    327\u001b[39m         modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28mself\u001b[39m._load_sbert_model(\n\u001b[32m    328\u001b[39m             model_name_or_path,\n\u001b[32m    329\u001b[39m             token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m             config_kwargs=config_kwargs,\n\u001b[32m    337\u001b[39m         )\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         modules = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[32m    353\u001b[39m     modules = OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\gen\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:2112\u001b[39m, in \u001b[36mSentenceTransformer._load_auto_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs, has_modules)\u001b[39m\n\u001b[32m   2109\u001b[39m tokenizer_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **tokenizer_kwargs}\n\u001b[32m   2110\u001b[39m config_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **config_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m2112\u001b[39m transformer_model = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2120\u001b[39m pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\gen\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:102\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m    101\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\gen\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1156\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m model_type = config_class_to_model_type(\u001b[38;5;28mtype\u001b[39m(config).\u001b[34m__name__\u001b[39m)\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     tokenizer_class_py, tokenizer_class_fast = \u001b[43mTOKENIZER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1159\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\gen\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:815\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    813\u001b[39m         model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[mtype]\n\u001b[32m    814\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._load_attr_from_module(mtype, model_name)\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: <class 'transformers.models.vit.configuration_vit.ViTConfig'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "def load_images(folder):\n",
    "    images = []\n",
    "    names = []\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            img = Image.open(os.path.join(folder, file)).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            names.append(file)\n",
    "\n",
    "    return images, names\n",
    "\n",
    "model = SentenceTransformer(\"facebook/dino-vits16\")\n",
    "\n",
    "\n",
    "\n",
    "def image_to_vector(image):\n",
    "    return model.encode(image)\n",
    "images, names = load_images(r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\dev\\Photos\")\n",
    "\n",
    "vectors = [image_to_vector(img) for img in images]\n",
    "\n",
    "dimension = 512\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine\n",
    "\n",
    "\n",
    "vectors_np = np.array(vectors).astype(\"float32\")\n",
    "index.add(vectors_np)\n",
    "\n",
    "print(\"Total images stored:\", index.ntotal)\n",
    "faiss.write_index(index, \"image_vectors.faiss\")\n",
    "import json\n",
    "\n",
    "with open(\"metadata.json\", \"w\") as f:\n",
    "    json.dump(names, f)\n",
    "query_img = Image.open(r\"C:\\Users\\kumaw\\Downloads\\2025102916413218.jpg\").convert(\"RGB\")\n",
    "query_vector = image_to_vector(query_img)\n",
    "\n",
    "D, I = index.search(\n",
    "    np.array([query_vector]).astype(\"float32\"),\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"Most similar images:\")\n",
    "for idx in I[0]:\n",
    "    print(names[idx])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb068fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Similar images saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import shutil\n",
    "\n",
    "SOURCE_DIR = r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\dev\\Photos\"\n",
    "DEST_DIR = r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\dev\\similar_results1111\"\n",
    "\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "for idx in I[0]:\n",
    "    src_path = os.path.join(SOURCE_DIR, names[idx])\n",
    "    dst_path = os.path.join(DEST_DIR, names[idx])\n",
    "\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"âœ… Similar images saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba25ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\gen\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kumaw\\.cache\\huggingface\\hub\\models--sentence-transformers--clip-ViT-L-14. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load images\n",
    "# -----------------------------\n",
    "def load_images(folder):\n",
    "    images = []\n",
    "    names = []\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            img = Image.open(os.path.join(folder, file)).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            names.append(file)\n",
    "\n",
    "    return images, names\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load MODEL (DINO v2)\n",
    "# -----------------------------\n",
    "model = SentenceTransformer(\"clip-ViT-L-14\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Image â†’ Vector (NORMALIZED)\n",
    "# -----------------------------\n",
    "def image_to_vector(image):\n",
    "    vec = model.encode(image)\n",
    "    vec = vec / np.linalg.norm(vec)   # ðŸ”¥ mandatory normalization\n",
    "    return vec\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset images\n",
    "# -----------------------------\n",
    "images, names = load_images(\n",
    "    r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\dev\\Photos\"\n",
    ")\n",
    "\n",
    "vectors = [image_to_vector(img) for img in images]\n",
    "\n",
    "# -----------------------------\n",
    "# FAISS index (CORRECT DIM)\n",
    "# -----------------------------\n",
    "dimension = 384  # ðŸ”¥ DINO output dimension\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "vectors_np = np.array(vectors).astype(\"float32\")\n",
    "index.add(vectors_np)\n",
    "\n",
    "print(\"Total images stored:\", index.ntotal)\n",
    "\n",
    "faiss.write_index(index, \"image_vectors.faiss\")\n",
    "\n",
    "with open(\"metadata.json\", \"w\") as f:\n",
    "    json.dump(names, f)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# QUERY IMAGE\n",
    "# -----------------------------\n",
    "query_img = Image.open(\n",
    "    r\"C:\\Users\\kumaw\\Downloads\\2025102916413218.jpg\"\n",
    ").convert(\"RGB\")\n",
    "\n",
    "query_vector = image_to_vector(query_img)\n",
    "\n",
    "D, I = index.search(\n",
    "    np.array([query_vector]).astype(\"float32\"),\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"\\nMost similar images:\")\n",
    "for score, idx in zip(D[0], I[0]):\n",
    "    print(names[idx], \" | similarity:\", round(float(score), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297c348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'feature_extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Load dataset images\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     46\u001b[39m images, names = load_images(\n\u001b[32m     47\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mGenerativeAi\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mGen ai\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1-Basics+Of+Langchain\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdev\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mPhotos\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m vectors = \u001b[43m[\u001b[49m\u001b[43mimage_to_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# FAISS index\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     56\u001b[39m dimension = vectors[\u001b[32m0\u001b[39m].shape[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# ðŸ”¥ auto-detect (SAFE)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Load dataset images\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     46\u001b[39m images, names = load_images(\n\u001b[32m     47\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mGenerativeAi\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mGen ai\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1-Basics+Of+Langchain\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdev\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mPhotos\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m vectors = [\u001b[43mimage_to_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# FAISS index\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     56\u001b[39m dimension = vectors[\u001b[32m0\u001b[39m].shape[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# ðŸ”¥ auto-detect (SAFE)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mimage_to_vector\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimage_to_vector\u001b[39m(image):\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# ðŸ”¥ InferenceClient does NOT support encode()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     vec = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_extraction\u001b[49m(image)\n\u001b[32m     37\u001b[39m     vec = np.array(vec).flatten().astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m     vec = vec / np.linalg.norm(vec)  \u001b[38;5;66;03m# mandatory normalization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\gen\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1964\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1963\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1965\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'SentenceTransformer' object has no attribute 'feature_extraction'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# -----------------------------\n",
    "# Load MODEL (Multimodal Embedding)\n",
    "# -----------------------------\n",
    "model = SentenceTransformer(\"clip-ViT-B-32\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load images\n",
    "# -----------------------------\n",
    "def load_images(folder):\n",
    "    images = []\n",
    "    names = []\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            img = Image.open(os.path.join(folder, file)).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            names.append(file)\n",
    "\n",
    "    return images, names\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Image â†’ Vector (NORMALIZED)\n",
    "# -----------------------------\n",
    "def image_to_vector(image):\n",
    "    # ðŸ”¥ InferenceClient does NOT support encode()\n",
    "    vec = model.feature_extraction(image)\n",
    "\n",
    "    vec = np.array(vec).flatten().astype(\"float32\")\n",
    "    vec = vec / np.linalg.norm(vec)  # mandatory normalization\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset images\n",
    "# -----------------------------\n",
    "images, names = load_images(\n",
    "    r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\dev\\Photos\"\n",
    ")\n",
    "\n",
    "vectors = [image_to_vector(img) for img in images]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# FAISS index\n",
    "# -----------------------------\n",
    "dimension = vectors[0].shape[0]  # ðŸ”¥ auto-detect (SAFE)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "vectors_np = np.array(vectors).astype(\"float32\")\n",
    "index.add(vectors_np)\n",
    "\n",
    "print(\"Total images stored:\", index.ntotal)\n",
    "\n",
    "faiss.write_index(index, \"image_vectors.faiss\")\n",
    "\n",
    "with open(\"metadata.json\", \"w\") as f:\n",
    "    json.dump(names, f)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# QUERY IMAGE\n",
    "# -----------------------------\n",
    "query_img = Image.open(\n",
    "    r\"C:\\Users\\kumaw\\Downloads\\2025102916413218.jpg\"\n",
    ").convert(\"RGB\")\n",
    "\n",
    "query_vector = image_to_vector(query_img)\n",
    "\n",
    "D, I = index.search(\n",
    "    np.array([query_vector]).astype(\"float32\"),\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"\\nMost similar images:\")\n",
    "for score, idx in zip(D[0], I[0]):\n",
    "    print(names[idx], \"| similarity:\", round(float(score), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# -----------------------------\n",
    "# AUTO SAVE SIMILAR IMAGES\n",
    "# -----------------------------\n",
    "SOURCE_DIR = r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\dev\\Photos\"\n",
    "SAVE_DIR = r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\dev\\similar_images\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.75   # adjust if needed\n",
    "\n",
    "saved_count = 0\n",
    "\n",
    "for score, idx in zip(D[0], I[0]):\n",
    "    if score >= SIMILARITY_THRESHOLD:\n",
    "        src_path = os.path.join(SOURCE_DIR, names[idx])\n",
    "\n",
    "        # save with similarity score in name\n",
    "        dst_name = f\"{round(float(score), 3)}_{names[idx]}\"\n",
    "        dst_path = os.path.join(SAVE_DIR, dst_name)\n",
    "\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        saved_count += 1\n",
    "\n",
    "print(f\"\\nâœ… {saved_count} similar images saved in:\\n{SAVE_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e0a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc9cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "DSC_2546 copy.jpg (1).jpeg\n",
      "DSC_2546 copy.jpg.jpeg\n",
      "DSC_8701.JPG.jpeg\n",
      "DSC_5068.JPG.jpeg\n",
      "DSC_5517.JPG.jpeg\n"
     ]
    }
   ],
   "source": [
    "query_img = Image.open(r\"D:\\GenerativeAi\\Gen ai\\1-Basics+Of+Langchain\\3.2-DataIngestion\\page4_img2.png\").convert(\"RGB\")\n",
    "query_vector = image_to_vector(query_img)\n",
    "\n",
    "D, I = index.search(\n",
    "    np.array([query_vector]).astype(\"float32\"),\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"Most similar images:\")\n",
    "for idx in I[0]:\n",
    "    print(names[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3892746",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images\\\\DSC_2546 copy.jpg (1).jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     src_path = os.path.join(SOURCE_DIR, names[idx])\n\u001b[32m     11\u001b[39m     dst_path = os.path.join(DEST_DIR, names[idx])\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSimilar images saved successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:419\u001b[39m, in \u001b[36mcopy\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(dst):\n\u001b[32m    418\u001b[39m     dst = os.path.join(dst, os.path.basename(src))\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m copymode(src, dst, follow_symlinks=follow_symlinks)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:256\u001b[39m, in \u001b[36mcopyfile\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    254\u001b[39m     os.symlink(os.readlink(src), dst)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    258\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[32m    259\u001b[39m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'images\\\\DSC_2546 copy.jpg (1).jpeg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "SOURCE_DIR = \"images\"\n",
    "DEST_DIR = \"similar_results\"\n",
    "\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "for idx in I[0]:\n",
    "    src_path = os.path.join(SOURCE_DIR, names[idx])\n",
    "    dst_path = os.path.join(DEST_DIR, names[idx])\n",
    "\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Similar images saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191665c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
